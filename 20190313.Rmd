---
title: "March 13"
output: html_notebook
---

## Packages

```{r}
library(ggbiplot)
library(MASS)
library(tidyverse)
library(ggrepel)
library(ggmap)
```

## Discriminant analysis continued

### Leisure activities

This is what we had last week:

```{r}
my_url="http://www.utsc.utoronto.ca/~butler/d29/profile.txt"
active=read_delim(my_url," ")
active
active.1=lda(job~reading+dance+tv+ski,data=active)
```

and we decided that the jobs were pretty distinguishable by how the people in those jobs rated the leisure activities:

```{r}
p=predict(active.1)
d=cbind(active, p)
d
```

```{r}
ggplot(d, aes(x=x.LD1, y=x.LD2, colour=job, label=job)) + geom_point() + geom_text_repel()
```

People that like dancing are on the left, people that like TV-watching are at the bottom:

```{r}
active.1
```

The predictions got everyone right:

```{r}
with(d, table(job, class))
```


Now, we predicted group membership from *same* data used to make LDs: *cheating*!

A more honest way to go is to make LDs from all the data points *except one*, and then predict the one you left out. This is called **cross-validation**. That goes this way:

```{r}
active.2=lda(job~reading+dance+tv+ski,data=active, CV=T)
active.2
```

This produces everything `predict` would, but by cross-validation. Making a data frame of this doesn't work:

```{r, error=TRUE}
d2=cbind(active, active.2)
```

so we have to be a little more careful:

```{r}
d2=cbind(active, class=active.2$class, posterior=active.2$posterior)
d2
```

and let's round those posterior probabilities to 3 decimals while we're about it:

```{r}
d2 %>% mutate_at(vars(starts_with("posterior")), ~round(., 3)) -> d2
d2
```

How well do people get classified now?

```{r}
with(d2, table(job, class))
```

One of the bellydancers got classified as a politician. Was it a close call?

```{r}
d2 %>% filter(job != class)
```

This person was "certainly" a politician (except that they were not!). Where were they on our plot?

```{r}
ggplot(d, aes(x=x.LD1, y=x.LD2, colour=job, label=job)) + geom_point() + geom_text_repel()
```

If you take that unusual bellydancer away, the bellydancers are more up and left, and the one we left out looks more like a politician. 


### Peanuts revisited

```{r}
my_url="http://www.utsc.utoronto.ca/~butler/d29/peanuts.txt"
peanuts=read_delim(my_url," ")
peanuts
```

Three responses (cannot plot), two grouping variables.

Discriminant analysis can only handle one grouping variable, so create combination column first:

```{r}
(peanuts %>% unite(combo, variety, location) -> peanuts_combo)
```

How does `combo` "depend" on the other things?

```{r}
peanuts.1=lda(combo~y+smk+w, data=peanuts_combo)
peanuts.1
```

This time, have 3 LDs: three response variables, 6 combo groups, $\min(3, 6-1)=3$.

"Proportion of trace" says how relatively important each of the LDs is at distinguishing the groups. LD1 is very important, LD2 somewhat, LD3 not at all. Ignore LD3 from here.

To see what variables the LDs depend on, decide whether each coefficient is "clearly positive", "clearly negative", "close to zero". (Judgement call.)

For me:
- LD1 depends positively on `w`, negatively on `y`, not so much on `smk`.
- LD2 depends positively on `w`, not much on others.

That is, `smk` does very little to distinguish the groups.

Predictions:

```{r}
p=predict(peanuts.1)
cbind(peanuts_combo, p) %>% mutate_at(vars(starts_with("posterior")), ~round(., 2)) -> d
View(d)
```

Plot `LD1` and `LD2` with `combo` as a labelled scatterplot:

```{r}
ggplot(d, aes(x=x.LD1, y=x.LD2, colour=combo, label=combo))+
  geom_point()+geom_text_repel()
```


This plot suggests that the combos are mostly well distinguished. How are the predictions?


```{r}
with(d, table(combo, class))
```

One of the actual variety 6, location 2 was predicted to be a variety 5, location 1, but others all correct.

How clear-cut are these?

```{r}
d %>% select(combo, class, starts_with("posterior"))
```

Mostly clear (posterior prob near 1 for right combo, 0 otherwise). The wrong one was a very close call. The 5_1 and 6_2 seem the most confusible.

Bi-plot shows LDs and original variables:

```{r}
ggbiplot(peanuts.1, groups=factor(d$combo))
```

Points on the right are high on `w` and low on `y`; points on left vice versa.

Cross-validation: fitting and prediction together:

```{r}
peanuts.2=lda(combo~y+smk+w, data=peanuts_combo, CV=T)
cbind(peanuts_combo, class=peanuts.2$class, posterior=peanuts.2$posterior) -> cv
cv
with(cv, table(obs=combo, pred=class))
```

Predictions not so good this time. Posterior probabilities?

```{r}
cv %>% mutate_at(vars(contains("_")), ~round(.,3)) %>% select(-c(y, smk, w))
```

The wrong ones are more badly wrong this time. Why? Look again at LD plot:

```{r}
ggplot(d, aes(x=x.LD1, y=x.LD2, colour=combo, label=combo))+
  geom_point()+geom_text_repel()
```

Each prediction based on *omitting* the one being predicted. So if two plants for each combo a long way apart on plot a long way apart, each of them will probably be mistaken for something else. Eg. the `5_2` nearer the `8_1`s will almost certainly be classified as one of those. 

### Remote sensing of crops

Measure four variables `x1` through `x4` from aerial photographs. Use them to determine which of five crops are being grown in the fields being photographed.

```{r}
my_url="http://www.utsc.utoronto.ca/~butler/d29/remote-sensing.txt"
crops=read_table(my_url)
crops
```

Which variables are important?

```{r}
crops.1=lda(crop~x1+x2+x3+x4, data=crops)
crops.1
```

Should look at LD1 and LD2, but not the others.

LD1 depends mainly on `x1`, LD2 mainly on `x3`.

What we really care about is how distinguishable the crops are. So plot scores on LD1 and LD2, distinguished by crop:

```{r}
p=predict(crops.1)
cbind(crops, p) %>% mutate_at(vars(starts_with("posterior")), ~round(., 3)) -> d
d
```

```{r}
ggplot(d, aes(x=x.LD1, y=x.LD2, colour=crop))+geom_point()
```

Pretty awful! Maybe corn is distinguishable from the rest, and maybe (somewhat) soybeans, but clover is all over the place!

Biplot shows connection with original x's:

```{r}
ggbiplot(crops.1, groups=crops$crop)
```

`x1` distinguishes some of the crops:

```{r}
ggplot(crops, aes(x=crop, y=x1)) + geom_boxplot()
```

and maybe `x3` some of the others:

```{r}
ggplot(crops, aes(x=crop, y=x3)) + geom_boxplot()
```

but not very convincingly.

Expect the predictions to be pretty awful:

```{r}
with(d, table(crop, class))
```

Almost all of the corn are gotten right, and half of the soybeans (and surprisingly half of the clover). 

Alternatively:

```{r}
d %>% count(crop, class)
```

or,

```{r}
d %>% count(crop, class) %>% 
  spread(class, n, fill=0)
```

Expect the posterior probabilities to be pretty unclear:

```{r}
d %>% select(crop, class, starts_with("posterior")) -> d3
d3 %>% rename_at(vars(starts_with("posterior")), 
                 ~str_replace(., "posterior","p")) %>% View("posterior")
```

look at the clover (diverse) and the corn (compact).



Even the actual corn has a mostly less than 50% posterior probability of being corn, based on `x1` through `x4`. Some of the clover is likely clover, but some of it is more likely something else!


## Cluster analysis

slide 403

### one to ten

slides 405-407

slides 408-415

the dissimilarity data

```{r}
my_url="http://www.utsc.utoronto.ca/~butler/d29/languages.txt"
number.d=read_table(my_url)
number.d
```

making a distance object (for input to clustering)

```{r}
d = number.d %>%
select(-la) %>%
as.dist()
d
```

single-linkage (nearest-neighbour) clustering

```{r}
d.hc=hclust(d, method="single")
plot(d.hc)
```

The clustering process

```{r}
d.hc$labels %>% enframe()
```

```{r}
d.hc$merge
```

- languages 2 and 3 combined (Norwegian and Danish)
- languages 6 and 8 combined (French and Italian)
- language 7 (Spanish) combined with cluster formed at step 2 (French and Italian)
- language 1 (English) combined with cluster of step 1 (Norwegian and Danish)
...
- language 11 (Finnish) combined with cluster of step 9 (everything else).

Complete linkage:

```{r}
d.hc=hclust(d,method="complete")
plot(d.hc)
```

Requires *whole cluster* to be similar before joining.

Ward's method:

```{r}
d.hc=hclust(d,method="ward.D")
plot(d.hc)
```

Tends to form small clusters first, then join those.



Which clusters are the languages in? This one is Ward. I think three clusters is good. 
(Look for a vertical distance on the tree where the clustering doesn't change)

```{r}
cutree(d.hc,3)
```

or

```{r}
cutree(d.hc,3) %>% enframe(name="language", value="cluster")
```

Drawing the tree with a given number of clusters:

```{r}
plot(d.hc)
rect.hclust(d.hc,3)
```

How did I make those dissimilarities?

```{r}
my_url="http://www.utsc.utoronto.ca/~butler/d29/one-ten.txt"
lang=read_delim(my_url," ")
lang
```

Extract first letter. To do that, get all the number names in one column:

```{r}
lang %>% mutate(number=1:10) %>%
  gather(language,name,-number) %>%
  mutate(first=str_sub(name,1,1)) -> lang_long
lang_long
```

Dissimilarity between English and Norwegian: the number of first letters that are different.

English

```{r}
(lang_long %>% filter(language=="en") -> english)
```

Norwegian

```{r}
(lang_long %>% filter(language=="no") -> norwegian)
```

Put them side by side, matched by number:

```{r}
english %>% left_join(norwegian, by="number")
```

Count the different ones:

```{r}
english %>% left_join(norwegian, by="number") %>%
  mutate(different=(first.x!=first.y)) %>%
  summarize(diff=sum(different))
```

1 and 8 start with different letter, rest same.

Function to do this:

```{r}
countdiff=function(lang.1,lang.2,d) {
  lang1d=d %>% filter(language==lang.1)
  lang2d=d %>% filter(language==lang.2)
  lang1d %>% left_join(lang2d, by="number") %>%
  mutate(different=(first.x!=first.y)) %>%
  summarize(diff=sum(different)) %>%
  pull(diff)
}
```

Test:

```{r}
countdiff("en", "no", lang_long)
```

To do with all pairs of languages, first need the languages:

```{r}
languages=names(lang)
languages
```

then all pairs of them:

```{r}
pairs=crossing(lang=languages, lang2=languages)
pairs
```

For each lang and lang2 in parallel, run countdiff:

```{r}
(pairs %>% 
  mutate(diff=map2_int(lang, lang2, ~countdiff(.x, .y, lang_long))) -> thediffs)
```

make square table:

```{r}
thediffs %>% spread(lang2, diff)
```

which was where we began.

### Birth, death and infant mortality

slide 436

```{r}
my_url="http://www.utsc.utoronto.ca/~butler/d29/birthrate.txt"
vital=read_table(my_url)
vital
```

slide 439 about standardizing

```{r}
(vital %>% mutate_if(is.numeric, ~scale(.)) -> vital.s)
```

Try for 3 clusters (why? later.)

```{r}
vital.s %>% select(-country) %>% kmeans(3) -> vital.km3
vital.km3
```

so obtain things by name:

```{r}
vital.km3$size
```

```{r}
vital.km3$cluster
```

```{r}
vital.km3$centers
```

```{r}
vital.km3$withinss
```

```{r}
d=cbind(vital.s, vital.km3$cluster)
View(d, "K-means clustering")
```

slide 447 re randomization and `nstart`

To figure out how many clusters, work out (for each candidate #clusters) a measure of goodness of fit for that many clusters, such as `tot.withinss`:




```{r}
vital.s %>% select(-country) -> vv
tibble(clusters=2:20) %>% mutate(km=map(clusters, ~kmeans(vv, ., nstart=20))) %>% 
  mutate(ss=map_dbl(km, "tot.withinss")) -> twss
twss
```

plot against number of clusters, joined by lines, called **scree plot**:

```{r}
ggplot(twss, aes(x=clusters, y=ss)) + geom_point() + geom_line()
```

Look for "elbow" sticking out to *bottom*, eg at 6 clusters.

Suggests that 6 clusters would be good:

```{r}
vital.s %>% select(-country) %>% kmeans(6, nstart=20) -> vital.km6
d=cbind(vital.s, cluster=vital.km6$cluster)
View(d, "six clusters")
```

Compare three-cluster and six-cluster solutions:

```{r}
table(vital.km3$cluster, vital.km6$cluster)
```

Clusterings are mostly but not completely similar:

- most of old cluster 1 is new cluster 6, but some to cluster 5
- most of old cluster 2 is new cluster 2, but some to cluster 4
- most of old cluster 3 is new cluster 4, but some to cluster 4 or 6.

(Slightly different from one in slides: randomization.)

To get picture out of K-means, run a discriminant analysis on clusters as if known groups:

```{r}
vital.lda=lda(cluster~birth+death+infant,data=d)
vital.lda
```

Ignore LD3, so can plot LD1 vs LD2:

```{r}
p=predict(vital.lda)
dd=cbind(d, p)
dd
```

```{r}
ggplot(dd, aes(x=x.LD1, y=x.LD2, colour=factor(cluster), label=country)) + geom_point() + geom_text_repel(size=2) + guides(colour=F) 
ggsave("graph.png")
```

or, bigger, slide 465 (upside down vs. this).

Hockey league

```{r}
my_url="http://www.utsc.utoronto.ca/~butler/d29/ontario-road-distances.csv"
ontario=read_csv(my_url)
ontario
```

These are distances, so back to `hclust`.

Can we make a map first?

**get your own Google Maps API key** and use it here. The free one is fine. I saved mine in a secret place.

```{r}
api_key=Sys.getenv("api_key")
register_google(api_key)
```

Get the place names and look up lat-lon:

```{r}
tibble(place=ontario$place) %>% 
  mutate(withon=str_c(place, " ON")) %>% 
  mutate_geocode(withon) -> places
places
```

```{r}
map=get_map("Barrie", zoom=6)
```

```{r}
ggmap(map) + geom_point(data=places, aes(x=lon, y=lat)) + geom_text_repel(data=places, aes(label=place))
```


```{r}
ontario.d = ontario %>% select(-place) %>% as.dist()
ontario.hc=hclust(ontario.d,method="ward.D")
plot(ontario.hc)
rect.hclust(ontario.hc,4)
```

One cluster is way too big. Try more clusters:

```{r}
plot(ontario.hc)
rect.hclust(ontario.hc,7)
```

OK, now we can get creative:

- Thunder Bay, Sault Ste Marie, Huntsville, North Bay as "northern division"
- Brockville, Cornwall, Ottawa, Peterborough, Belleville, Kingston: "east"
- Windsor, London, Sarnia: "west"
- the rest: "central".

Now, to get them on map. Get the 7-cluster solution and rejuggle:

```{r}
cutree(ontario.hc, 7) %>%  enframe(name="place", value="clus1") %>% 
  mutate(division=case_when(clus1 == 1        ~ "Central",
                            clus1 %in% c(2,3) ~ "Eastern",
                            clus1 == 5        ~ "Western",
                            TRUE              ~ "Northern"
                           )) -> divs
divs
```

Look up lats and longs of those places (which we already got):

```{r}
divs %>% left_join(places, by="place") -> with_divs
with_divs
```

plot map with division coloured

```{r}
ggmap(map) + geom_point(data=with_divs, aes(x=lon, y=lat, colour=division)) + geom_text_repel(data=places, aes(label=place), size=2) +
  guides(colour=F)
ggsave("map2.png")
```



