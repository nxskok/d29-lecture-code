---
title: "March 27"
output: html_notebook
---

## packages

```{r}
library(ggbiplot)
library(tidyverse)
library(ggrepel)
```

## reminder: track running records

```{r}
my_url="http://www.utsc.utoronto.ca/~butler/d29/men_track_field.txt"
track=read_table(my_url)
track
```

8 variables. Can we make 2 composite variables out of these to understand the data more easily?

```{r}
track_num = track %>% select_if(is.numeric)
track.pc=princomp(track_num, cor=T)
summary(track.pc)
```

Two components explain most (94%) of the variability (scree plot also suggests two).

Plot component scores for first two components labelled by the country they belong to:

```{r fig.height=15, fig.width=15}
track.pc$scores %>% as_tibble() %>%
  select(1:2) %>% 
  bind_cols(country=track$country) -> d
d
ggplot(d,aes(x=Comp.1, y=Comp.2, label=country))+
  geom_point()+geom_text_repel()+
  coord_fixed()
```

Good (left) vs bad (right); better at distance running (top) vs sprinting (bottom).

## principal components from correlation matrix

store in file and read in:

```{r}
my_url="http://www.utsc.utoronto.ca/~butler/d29/cov.txt"
mat=read_table(my_url,col_names=F)
mat
```

correlations between three variables X1-X3.

```{r}
mat %>% as.matrix() %>%
princomp(covmat=.) -> mat.pc
summary(mat.pc)
```

there is one component, which we can also see with a screeplot:

```{r}
ggscreeplot(mat.pc)
```

Clear elbow at 2, so one component.

Compare original correlation matrix:

```{r}
mat
```

with loadings

```{r}
mat.pc$loadings
```

component 1 is large when X1 and X2 are large, X3 small. Small for opposite.

Correlation matrix says that one of these is likely to happen:

```{r}
mat
```


so data are basically one-dimensional.

see slide 579.

## factor analysis

slide 581

### children test scores

slide 583

read in data (correlation matrix)

```{r}
my_url="http://www.utsc.utoronto.ca/~butler/d29/rex2.txt"
kids = read_delim(my_url," ")
kids
```

first run principal components to get scree plot

```{r}
kids %>%
  select_if(is.numeric) %>%
  as.matrix() %>%
  princomp(covmat=.) -> kids.pc
ggscreeplot(kids.pc)
```

Elbow at 3, two components / factors.

```{r}
kids.pc$loadings
```

- first component has some of everything
- second component mainly `add` and `dots`
- not clear

  slide 587
  
```{r}
kids %>%
  select_if(is.numeric) %>%
  as.matrix() -> km
km
rownames(km)=colnames(km)
km
km2=list(cov=km,n.obs=145)
kids.f2=factanal(factors=2,covmat=km2)
```

Uniquenesses (slide 589)

```{r}
kids.f2$uniquenesses
```

Loadings

```{r}
kids.f2$loadings
```

slide 590

Are two factors enough?

```{r}
kids.f2$PVAL
```

2 factors not rejected, so enough.

Would  1 factor have been enough?

```{r}
kids.f1=factanal(factors=1,covmat=km2)
kids.f1$PVAL
```

No: we need 2 factors.

### track running records revisited

```{r}
my_url="http://www.utsc.utoronto.ca/~butler/d29/men_track_field.txt"
track=read_table(my_url)

my_url="http://www.utsc.utoronto.ca/~butler/d29/isocodes.csv"
iso=read_csv(my_url)

track_num = track %>% select_if(is.numeric)
track.pc=princomp(track_num,cor=T)
ggbiplot(track.pc,labels=track$country)
```


principal component dimensions are good-bad (left-right), sprinting-distance (up-down).

What happens when we do factor analysis? Slide 594.

```{r}
track %>% select_if(is.numeric) %>%
factanal(2,scores="r") -> track.f
```

Biplot (not ggplot):

```{r fig.height=12, fig.width=12}
biplot(track.f$scores,track.f$loadings, xlabs=track$country)
```

100m points up, and marathon points across. Suggests two dimensions will be more clearly sprinting and distance running. Note "rotation" of variables on biplot.

```{r}
track.f$loadings
```

factor 1, highest loadings on long distance, factor 2 highest loadings on sprinting.

Which countries are best at distance running or sprinting? Most negative scores on factor 1, 2 respectively.

Make data frame with  country abbreviations and factor scores, and add country names. (Compare what happens if I use `cbind`.)

```{r}
data.frame(country=track$country, track.f$scores) %>% 
  left_join(iso,by=c("country"="ISO2")) -> d
d
```

best distance-running countries:

```{r}
d %>% arrange(Factor1)
```

best sprinting countries:

```{r}
d %>% arrange(Factor2)
```

### BEM sex role inventory

slide 602-603.

```{r}
my_url="http://www.utsc.utoronto.ca/~butler/d29/factor.txt"
bem=read_tsv(my_url)
bem
```

Traits have abbreviated names. Scores on 7-point scale: 1=strongly disagree, 7=strongly agree.

Side track: make a lookup table. First need all (distinct) short names:

```{r}
bem %>% gather(trait, score, -subno) %>% distinct(trait) %>% write_csv("traits.csv")
```

then open `traits.csv` with spreadsheet, add new column with full traits in `trait_full`.

then read this back in:

```{r}
my_url="http://www.utsc.utoronto.ca/~butler/d29/traits.csv"
traits=read_csv(my_url)
traits
```






use principal components to decide how many factors:

```{r}
bem %>% select(-subno) %>% princomp(cor=T) -> bem.pc
ggscreeplot(bem.pc)
```

Hard to see. Looking for a reasonably small number of factors:

```{r}
ggscreeplot(bem.pc)+scale_x_continuous(limits=c(0,8))
```

Maybe elbows at 3 (2 factors) or 6 (5 factors).

But:

```{r}
summary(bem.pc)
```

2 factors explains only 29% of variability, and 5 only 43%. We are not going to be able to do so well here.

Biplot:

```{r fig.height=12, fig.width=12}
ggbiplot(bem.pc,alpha=0.3)
```

A lot of variables point to 1 o'clock or 4 o'clock. A rotation might point them up-down or left-right.

2 factors (may be bad):

```{r}
bem.2 = bem %>% select(-subno) %>% factanal(factors=2, scores="r")
bem.2
```

Did we rotate?

```{r fig.height=12, fig.width=12}
biplot(bem.2$scores,bem.2$loadings, xlabs=bem$subno)
```

Most of the red arrows now go up or right, so factors ought to have (somewhat) clearer structure.


What is factor 1 made of?

Trickiness first:

```{r}
bem.2$loadings
class(bem.2$loadings) # not a data frame
bem.2$loadings %>% unclass() %>% as.data.frame() %>% rownames_to_column("trait") -> bem.loadings
bem.loadings
```

Look up full trait names:

```{r}
(bem.loadings %>% left_join(traits) -> bem.loadings)
```


```{r}
bem.loadings %>% arrange(desc(abs(Factor1)))
```

and factor 2?

```{r}
bem.loadings %>% arrange(desc(abs(Factor2)))
```

Clearly two very different types of traits.

Go back to biplot, now looking for unusual *individuals*. The numbers are subject numbers in `subno`:

```{r fig.height=12, fig.width=12}
biplot(bem.2$scores,bem.2$loadings, xlabs=bem$subno)
```

On factor 2, 755 high and 534 low:

```{r}
bem %>% filter(subno %in% c(755, 534)) %>% gather(trait, score, -subno) %>% spread(subno, score) -> extreme_2
extreme_2
```

but we need to look at the columns *which load highly on factor 2*:

```{r}
bem.loadings
```

```{r}
extreme_2 %>% left_join(bem.loadings) %>% arrange(desc(abs(Factor2)))
```

Subject 755 scores highly on these traits (and even low on "masculine" with negative loading!)
Subject 534 scores at least relatively low on these traits.

Repeat with factor 1: 404 high and 708 low:

```{r}
bem %>% filter(subno %in% c(404, 708)) %>% gather(trait, score, -subno) %>% spread(subno, score) %>% 
  left_join(bem.loadings) %>% arrange(desc(abs(Factor1)))
```

subject 404 is a high scorer on everything (and a low scorer on "shy" with negative loading)
subject 708 is mainly a low scorer on these.

Try a crazy number of factors like 15(!)

```{r}
bem %>% select(-subno) %>%
  factanal(factors=15) -> bem.15
```

and then see what each factor has in it. Loadings first, with full trait names

```{r}
bem.15$loadings %>% unclass() %>% as.data.frame() %>% rownames_to_column("trait") %>% 
  left_join(traits) -> bem.loadings
bem.loadings
```

```{r}
bem.loadings %>% gather(factor, score, starts_with("Factor")) -> loadings.long
loadings.long
```

```{r}
high_loadings=function(factor_name) {
  loadings.long %>% filter(factor==factor_name) %>% 
  arrange(desc(abs(score)))
}
```

then do it for any factor

```{r}
factor_name="Factor15"
high_loadings(factor_name)
```

but, if we look at the uniquenesses:

```{r}
enframe(bem.15$uniquenesses, name="trait") %>% arrange(desc(value)) %>% left_join(traits)
```

some of these are still very high, so even 15 factors is not catching everything.


Time Series
-----------

## Packages

You might need to install these:

```{r, eval=F}
install.packages("ggfortify")
install.packages("forecast")
install.packages("devtools")
devtools::install_github("nxskok/mkac")
```


```{r}
library(tidyverse)
library(mkac) 
library(ggfortify)
library(forecast)
```


## Time trends

* Assess existence or nature of time trends with:
  * correlation
  * regression ideas.
  
### World mean temperatures


Global mean temperature every year since 1880:

```{r}
my_url="http://www.utsc.utoronto.ca/~butler/d29/temperature.csv"
temp=read_csv(my_url)
ggplot(temp, aes(x=year, y=temperature)) + geom_point() + geom_smooth()
```


Examining trend

* Temperatures increasing on average over time, but pattern very irregular.

### Kendall correlation

Kendall (rank) correlation, which just tests for monotone trend (anything upward, anything downward) and is resistant to outliers:

```{r}
with(temp, cor.test(temperature,year,method="kendall"))
```

### Mann-Kendall

Kendall correlation with time called **Mann-Kendall**.

Also in my package `mkac`:

```{r}
kendall_Z_adjusted(temp$temperature)
```

P-value is very small, but (correctly) adjusted one not as small as before because of *autocorrelation* (see later). Idea: observations close together in time are correlated with each other, so observations not independent. This is correction for that.

### Examining rate of change

* Having seen that there *is* a change, question is "how fast is it?"
* Examine **Theil-Sen slope**: resistant to outliers, based on medians
  
also from `mkac`:

```{r}
slope=theil_sen_slope(temp$temperature)
slope
```

this is degrees C per year. Doesn't seem like much, but over 130 years of data is

```{r}
130*slope
```



This assumes that the rate of change is same over all years, but trend seemed to be accelerating:

```{r}
ggplot(temp, aes(x=year, y=temperature)) + geom_point() + geom_smooth()
```

Look at pre-1970 and post-1970:

```{r}
temp %>% mutate(time_period=ifelse(year<=1970, "pre-1970", "post-1970")) %>% 
  nest(-time_period) %>% 
  mutate(theil_sen=map_dbl(data, ~theil_sen_slope(.$temperature)))
```

Theil-Sen slope is very nearly *four times* as big since 1970 vs. before.

## Actual time series

### The Kings of England

* Age at death of Kings and Queens of England since William the Conqueror (1066):

```{r}
my_url="http://www.utsc.utoronto.ca/~butler/d29/kings.txt"
kings=read_table(my_url, col_names=F)
kings.ts=ts(kings)
```

Data in one long column `X1`, so `kings` is data frame with one column. Turn into `ts` time series object.

```{r}
kings.ts
```


### Plotting a time series

`autoplot` from `ggfortify` gives time plot:

```{r,"Kings-Time-Series2"}
autoplot(kings.ts)
```

Comments

* "Time" here is order of monarch from William the Conqueror (1st) to George VI (last).

* Looks to be slightly increasing trend of age-at-death

* but lots of irregularity.


### Stationarity

A time series is **stationary** if:

* mean is constant over time
* variability constant over time and not changing with mean.

Kings time series seems to have:

* non-constant mean
* but constant variability
* not stationary.

Usual fix for non-stationarity is *differencing*: new series 2nd - 1st, 3rd - 2nd etc.

In R, `diff`:

```{r}
kings.diff.ts=diff(kings.ts)
```

Did differencing fix stationarity?

Looks stationary now:

```{r, "Differenced-Kings-Series"}
autoplot(kings.diff.ts)
```



### Births per month in New York City

from January 1946 to December 1959:

```{r}
my_url="http://www.utsc.utoronto.ca/~butler/d29/nybirths.txt"
ny=read_table(my_url,col_names=F)
ny.ts=ts(ny,freq=12,start=c(1946,1))
ny.ts
```

Note extras on `ts`:

* Time period is 1 year
* 12 observations per year (monthly) in `freq`
* First observation is 1st month of 1946 in `start`

Printing formats nicely.

Time plot

* Time plot shows extra pattern:

```{r,fig.cap=""}
autoplot(ny.ts)
```

Comments on time plot

* steady increase (after initial drop)
* repeating pattern each year (seasonal component).
* Not stationary.

Differencing the New York births

Does differencing help here?

```{r}
ny.diff.ts=diff(ny.ts)
autoplot(ny.diff.ts)
```

Looks stationary, but some regular spikes.

### Decomposing a seasonal time series

Observations for NY births were every month. Are things the same every year?

A visual (using original data):

```{r fig.height=12, fig.width=12}
ny.d=decompose(ny.ts) 
autoplot(ny.d)
```

Decomposition bits


Shows:

* original series
* a "seasonal" part: something that repeats every year
* just the trend, going steadily up (except at the start)
* random: what is left over ("remainder")

The seasonal part

Fitted seasonal part is same every year, births lowest in February and highest in July:


```{r}
ny.d$seasonal
```


```{r, echo=FALSE}
set.seed(457299)
```

## Time series basics

### White noise

Independent random normal. Knowing one value tells you nothing about the next. "Random" process.


```{r,"White-Noise"}
wn=rnorm(100)
wn.ts=ts(wn)
autoplot(wn.ts)
```




### Lagging a time series

This means moving a time series one (or more) steps back in time:

```{r}
x=rnorm(5)
tibble(x) %>% mutate(x_lagged=lag(x)) -> with_lagged
with_lagged
```

Gain a missing because there is nothing before the first observation.

Lagging white noise

```{r}
tibble(wn) %>% mutate(wn_lagged=lag(wn)) -> wn_with_lagged
ggplot(wn_with_lagged, aes(y=wn, x=wn_lagged))+geom_point()
with(wn_with_lagged, cor.test(wn, wn_lagged, use="c")) # ignore the missing value
```


Correlation with lagged series

If you know about white noise at one time point, you know *nothing* about it at the next. This is shown by the scatterplot and the correlation. 


On the other hand, this:

```{r}
tibble(age=kings$X1) %>% 
  mutate(age_lagged=lag(age)) -> kings_with_lagged
with(kings_with_lagged, cor.test(age, age_lagged))
```

If one value larger, the next value (a bit) more likely to be larger:

```{r}
ggplot(kings_with_lagged, aes(x=age_lagged, y=age)) + geom_point()
```

Two steps back:

```{r}
kings_with_lagged %>% 
  mutate(age_lag_2=lag(age_lagged)) %>% 
  with(., cor.test(age, age_lag_2))
```

Still a correlation two steps back, but smaller (and no longer significant).

### Autocorrelation

Correlation of time series with *itself* one, two,... time steps back is useful idea, called **autocorrelation**. Make a plot of it with `acf` and `autoplot`:

White noise:

```{r}
acf(wn.ts, plot=F) %>% autoplot()
```

No autocorrelations beyond chance, anywhere.

Autocorrelations work best on *stationary* series.

Kings, differenced

```{r}
acf(kings.diff.ts, plot=F) %>% autoplot()
```

Comments on autocorrelations of kings series

Negative autocorrelation at lag 1, nothing beyond that. 

* If one value of differenced series positive, next one most likely negative.
* If one king lives longer than predecessor, next one likely lives shorter. (Careful with interpretation: is of *differences* in age-at-death.)

NY births, differenced

```{r}
acf(ny.diff.ts, plot=F) %>% autoplot()
```

Lots of stuff:

* large positive autocorrelation at 1.0 years (July one year like July last year)
* large negative autocorrelation at 1 month.
* smallish but significant negative autocorrelation at 0.5 year = 6 months.
* Other stuff -- complicated.

### Souvenir sales

Monthly sales for a beach souvenir shop in Queensland, Australia:

```{r}

my_url="https://www.utsc.utoronto.ca/~butler/d29/souvenir.txt"
souv=read_table(my_url, col_names=F)
souv.ts=ts(souv,frequency=12,start=1987)
souv.ts
```

Plot of souvenir sales

```{r}
autoplot(souv.ts)
```

Several problems:

* Mean goes up over time
* Variability gets larger as mean gets larger
* Not stationary

Problem-fixing:

Fix non-constant variability first by taking logs:

```{r}
souv.log.ts=log(souv.ts)
autoplot(souv.log.ts)
```

Mean still not constant, so try taking differences:

```{r}
souv.log.diff.ts=diff(souv.log.ts)
autoplot(souv.log.diff.ts)
```

* Now stationary
* but clear seasonal effect.

Decomposing to see the seasonal effect

```{r fig.height=12, fig.width=12}
souv.d=decompose(souv.log.diff.ts)
autoplot(souv.d)
```

**Big** drop in one month's differences. Look at seasonal component to see which:

```{r}
souv.d$seasonal
```

January.

Autocorrelations:

```{r}
acf(souv.log.diff.ts, plot=F) %>% autoplot()
```

* Big positive autocorrelation at 1 year (strong seasonal effect)
* Small negative autocorrelation at 1 and 2 months.


### Moving average

A particular type of time series called a **moving average** or MA process captures idea of autocorrelations at a few lags but not at others.

Here's generation of MA(1) process, with autocorrelation at lag 1 but not otherwise:

```{r}
beta=1
tibble(e=rnorm(100)) %>% 
  mutate(e_lag=lag(e)) %>% 
  mutate(y=e+beta*e_lag) %>% 
  mutate(y=ifelse(is.na(y), 0, y)) -> ma
ma
```


* `e` contains independent "random shocks". 
* Start process at 0. 
* Then, each value of the time series has that time's random shock, plus a multiple of the last time's random shock. 
* `y[i]` has shock in common with `y[i-1]`; should be a lag 1 autocorrelation. 
* But `y[i]` has no shock in common with `y[i-2]`, so no lag 2 autocorrelation (or beyond).

```{r}
autoplot(ts(ma$y))
```


ACF for MA(1) process

```{r}
acf(ma$y, plot=F, na.rm=T) %>% autoplot()
```

Everything beyond lag 1 appears to be just chance.


### AR process

Another kind of time series is AR process, where each value depends on previous one, like this (loop):

```{r}
e=rnorm(100)
x=numeric(0)
x[1]=0
alpha=0.7
for (i in 2:100)
{
  x[i]=alpha*x[i-1]+e[i]
}
x
```

```{r}
autoplot(ts(x))
```


* Each random shock now only used for its own value of `x`
* but `x[i]` also depends on previous value `x[i-1]`
* so correlated with previous value
* *but* `x[i]` also contains multiple of `x[i-2]` and previous x's
* so all x's correlated, but autocorrelation dying away.

ACF for AR(1) series:

```{r}
acf(x, plot=F) %>% autoplot()
```

You can have a series with both AR and MA parts:

```{r}
arma=arima.sim(list(ar=c(0.6, 0.2),ma=1), n=100)
autoplot(arma)
```

```{r}
acf(arma)
```


### The new way of doing time series analysis (in R)

* Transform series if needed to get constant variability
* Use package `forecast`.
* Use function `auto.arima` to estimate what kind of series best fits data.
* Use `forecast` to see what will happen in future.

Anatomy of `auto.arima` output

```{r}
auto.arima(ma$y)
```

* ARIMA part tells you what kind of series you are estimated to have:
  * first number (first 0) is AR (autoregressive) part
  * second number (second 0) is amount of differencing
  * third number (1) is MA (moving average) part
  
* Below that, coefficients (with SEs)
* AICc is measure of fit (lower better)


### Doing it all the new way

#### White noise

```{r}
wn.aa=auto.arima(wn.ts)
wn.aa
```

Best fit *is* white noise (no AR, no MA, no differencing). 

Forecasts:

```{r}
forecast(wn.aa)
```

Forecasts all 0, since the past doesn't help to predict future.


#### MA(1)

```{r}
y.aa=auto.arima(ma$y)
y.aa
y.f=forecast(y.aa)
```

Plotting the forecasts for MA(1):

```{r}
autoplot(y.f)
```


#### AR(1)

```{r}
x.aa=auto.arima(x)
x.aa
```

Oops!

Got it wrong! Fit right AR(1) model:

```{r}
x.arima=arima(x,order=c(1,0,0))
x.arima
```

Forecasts for `x`:

```{r}
forecast(x.arima) %>% autoplot()
```


#### Kings

```{r}
kings.aa=auto.arima(kings.ts)
kings.aa
```

Kings forecasts:

```{r}
kings.f=forecast(kings.aa)
autoplot(kings.f) + labs(x="index", y= "age at death")
```


#### NY births


```{r}
ny.aa=auto.arima(ny.ts)
ny.aa
ny.f=forecast(ny.aa,h=36)
```

Going 36 time periods (3 years) into future.

Plotting the forecasts:

```{r}
autoplot(ny.f)+labs(x="time", y="births")
```


#### Log-souvenir sales

```{r}
souv.aa=auto.arima(souv.log.ts)
souv.aa
souv.f=forecast(souv.aa,h=27)
```



Plotting the forecasts

```{r}
autoplot(souv.f)
```


#### Global mean temperatures, revisited


```{r}
temp.ts=ts(temp$temperature,start=1880)
temp.aa=auto.arima(temp.ts)
temp.aa
```

Forecasts

```{r fig.height=9, fig.width=9}
temp.f=forecast(temp.aa)
autoplot(temp.f)+labs(x="year", y="temperature")
```


