---
title: "March 20"
output: html_notebook
---

## Packages

```{r}
library(MASS)
library(ggbiplot)
library(tidyverse)
library(ggrepel)
library(ggmap)
```


## Cluster analysis continued

### Birth, death and infant mortality (K-means)

slide 436

```{r}
my_url="http://www.utsc.utoronto.ca/~butler/d29/birthrate.txt"
vital=read_table(my_url)
vital
```

slide 439 about standardizing

```{r}
(vital %>% mutate_if(is.numeric, ~scale(.)) -> vital.s)
```

Try for 3 clusters (why? later.)

```{r}
vital.s %>% select(-country) %>% kmeans(3) -> vital.km3
vital.km3
```

so obtain things by name:

```{r}
vital.km3$size
```

```{r}
vital.km3$cluster
```

```{r}
vital.km3$centers
```

```{r}
vital.km3$withinss
```

```{r}
d=cbind(vital.s, vital.km3$cluster)
View(d, "K-means clustering")
```

slide 447 re randomization and `nstart`

To figure out how many clusters, work out (for each candidate #clusters) a measure of goodness of fit for that many clusters, such as `tot.withinss`:




```{r}
vital.s %>% select(-country) -> vv
tibble(clusters=2:20) %>% mutate(km=map(clusters, ~kmeans(vv, ., nstart=20))) %>% 
  mutate(ss=map_dbl(km, "tot.withinss")) -> twss
twss
```

plot against number of clusters, joined by lines, called **scree plot**:

```{r}
ggplot(twss, aes(x=clusters, y=ss)) + geom_point() + geom_line()
```

Look for "elbow" sticking out to *bottom*, eg at 6 clusters.

Suggests that 6 clusters would be good:

```{r}
vital.s %>% select(-country) %>% kmeans(6, nstart=20) -> vital.km6
d=cbind(vital.s, cluster=vital.km6$cluster)
View(d, "six clusters")
```

Compare three-cluster and six-cluster solutions:

```{r}
table(vital.km3$cluster, vital.km6$cluster)
```

Clusterings are mostly but not completely similar:

- most of old cluster 1 is new cluster 6, but some to cluster 5
- most of old cluster 2 is new cluster 2, but some to cluster 4
- most of old cluster 3 is new cluster 4, but some to cluster 4 or 6.

(Slightly different from one in slides: randomization.)

To get picture out of K-means, run a discriminant analysis on clusters as if known groups:

```{r}
vital.lda=lda(cluster~birth+death+infant,data=d)
vital.lda
```

Ignore LD3, so can plot LD1 vs LD2:

```{r}
p=predict(vital.lda)
dd=cbind(d, p)
dd
```

```{r}
ggplot(dd, aes(x=x.LD1, y=x.LD2, colour=factor(cluster), label=country)) + geom_point() + geom_text_repel(size=2) + guides(colour=F) 
ggsave("graph.png")
```

or, bigger, slide 465 (upside down vs. this).

### Hockey league (hierarchical)

```{r}
my_url="http://www.utsc.utoronto.ca/~butler/d29/ontario-road-distances.csv"
ontario=read_csv(my_url)
ontario
```

These are distances, so back to `hclust`.

Can we make a map first?

**get your own Google Maps API key** and use it here. The free one is fine. I saved mine in a secret place.

```{r}
api_key=Sys.getenv("api_key")
register_google(api_key)
```

Get the place names and look up lat-lon:

```{r}
tibble(place=ontario$place) %>% 
  mutate(withon=str_c(place, " ON")) %>% 
  mutate_geocode(withon) -> places
places
```

```{r}
map=get_map("Barrie", zoom=6)
```

```{r}
ggmap(map) + geom_point(data=places, aes(x=lon, y=lat)) + geom_text_repel(data=places, aes(label=place))
```


```{r}
ontario.d = ontario %>% select(-place) %>% as.dist()
ontario.hc=hclust(ontario.d,method="ward.D")
plot(ontario.hc)
rect.hclust(ontario.hc,4)
```

One cluster is way too big. Try more clusters:

```{r}
plot(ontario.hc)
rect.hclust(ontario.hc,7)
```

OK, now we can get creative:

- Thunder Bay, Sault Ste Marie, Huntsville, North Bay as "northern division"
- Brockville, Cornwall, Ottawa, Peterborough, Belleville, Kingston: "east"
- Windsor, London, Sarnia: "west"
- the rest: "central".

Now, to get them on map. Get the 7-cluster solution and rejuggle:

```{r}
cutree(ontario.hc, 7) %>%  enframe(name="place", value="clus1") %>% 
  mutate(division=case_when(clus1 == 1        ~ "Central",
                            clus1 %in% c(2,3) ~ "Eastern",
                            clus1 == 5        ~ "Western",
                            TRUE              ~ "Northern"
                           )) -> divs
divs
```

Look up lats and longs of those places (which we already got):

```{r}
divs %>% left_join(places, by="place") -> with_divs
with_divs
```

plot map with division coloured

```{r}
ggmap(map) + geom_point(data=with_divs, aes(x=lon, y=lat, colour=division)) + geom_text_repel(data=places, aes(label=place), size=2) +
  guides(colour=F)
ggsave("map2.png")
```



## principal components

- Find combinations of variables that distinguish the *individuals*.
- See whether a small number of such components explains most of the variability in the data
- if 2 components good, can plot them


### Two test scores for 8 people

```{r}
my_url="http://www.utsc.utoronto.ca/~butler/d29/test12.txt"
test12=read_table2(my_url)
test12
```

2 variables: plot

```{r}
ggplot(test12,aes(x=first,y=second,label=id))+
geom_point()+geom_text_repel()+geom_smooth(se=F)
```

grab only the numeric columns

```{r}
test12_numbers = test12 %>% select_if(is.numeric)
```

find correlation matrix

```{r}
cor(test12_numbers)
```

run principal components

```{r}
test12.pc = test12_numbers %>% princomp(cor=T)
summary(test12.pc)
```


Scree plot is kind of dumb:

```{r}
ggscreeplot(test12.pc)
```

imagine it continuing to rightat zero: 2 is a big below, so take $2-1=1$ component. 

Note difference from discriminant analysis: want last one *on mountain*, not first one on scree, so go back one from elbow.

How are PCs related to test scores?

```{r}
test12.pc$loadings
```

the first one is (scaled) sum of two standardized test scores. That's basically how individuals differ.

Component scores 

```{r}
d=cbind(test12,test12.pc$scores) 
d
```

- Person A is a low scorer
- person D is a high scorer
- person E is an average scorer

an odd graph: test scores with component 1 score as *colour*:

```{r}
ggplot(d, aes(x=first, y=second, colour=Comp.1)) + geom_point()
```

The lighter blue (higher component 1 score) are higher test scorers, the darker blue the lower test scores.

plot of scores

```{r}
ggplot(d,aes(x=Comp.1,y=Comp.2,label=id))+
geom_point()+geom_text_repel()
```

In so far as component 2 explains anything, it is the *difference* in test scores: positive = relatively better on test 1 (thus A did relatively better on test 2).

But this exaggerates how much the people vary on component 2 (note the scales). Should make axes the same scales:

```{r}
ggplot(d,aes(x=Comp.1,y=Comp.2,label=id))+
geom_point()+geom_text_repel()+
coord_fixed()
```

test scores really vary along *one* dimension.

#### Biplot

Individuals and component scores with original variables labelled

```{r}
ggbiplot(test12.pc,labels=test12$id)
```

High scorers on both tests are on the *right*, low scores on the left.

### Track running data

```{r}
my_url="http://www.utsc.utoronto.ca/~butler/d29/men_track_field.txt"
track=read_table(my_url)
track
```

Some of these are seconds, some are minutes. Countries are (usually) Internet domain codes (ISO 2-letter codes):

```{r}
my_url="http://www.utsc.utoronto.ca/~butler/d29/isocodes.csv"
iso=read_csv(my_url)
iso
```

slide 557

#### fit and examine principal components

```{r}
track_num = track %>% select_if(is.numeric)
track.pc=princomp(track_num,cor=T)
summary(track.pc)
```

Scree plot

```{r}
ggscreeplot(track.pc)
```

Elbows at 2 (one component) and 3 (two components).

Two components explain 94% of variability (vs 83% for one), so go with two. 

Taking more components will increase % variability explained, but not much, so stopping at two makes sense. (Slide 560.)


How do components depend on original variables?

```{r}
track.pc$loadings
```

Component 1 has basically equal weights all the way down: component 1 score is high if all the distance records are high (that is, if country *bad* at running). 

Component 2 distinguishes sprinting (short-distance running) from long-distance running: component 2 score is high if country bad at sprinting / good at distance running.

Make data frame of component scores and country names:

```{r}
track.pc$scores %>% as_tibble() %>%
  select(1:2) %>% 
  bind_cols(country=track$country) -> d
d
```

Plot with countries labelled (I resized this)

```{r fig.height=15, fig.width=15}
ggplot(d,aes(x=Comp.1,y=Comp.2,
label=country))+
geom_point()+geom_text_repel()+
coord_fixed()
```

Let's look up some countries:

```{r}
iso
```



Bad running countries

```{r}
v=c("ck", "ws")
tibble(want=v) %>% left_join(iso, by=c("want"="ISO2"))
```

interlude: write a function to do this

```{r}
lookup=function(v, iso) {
  tibble(want=v) %>% left_join(iso, by=c("want"="ISO2"))
}
lookup(v, iso)
```

Good running countries

```{r}
v=c("de", "au", "uk", "it", "us", "ru")
lookup(v, iso)
```

good at distance running relative to sprinting

```{r}
w=c("nz", "ir", "tr", "pt", "mx", "no", "kp", "ke")
lookup(w, iso)
```

good at sprinting relative to distance running

```{r}
w=c("us", "ru", "it", "bm", "do")
lookup(w, iso)
```

biplot

```{r fig.height=12, fig.width=12}
ggbiplot(track.pc,labels=track$country)
```

countries at the "front" of the arrows have *high* record times (bad), eg Cook Islands for 100m, Western Samoa for marathon.

countries at the opposite ends of the arrows (extended backwards) have *low* record times (good), eg. US for 100m, New Zealand for marathon.

Picture shows: good at running (left) vs bad at running (right); relatively good at distance running (top), good at sprinting (bottom).

Best 8 running countries?

```{r}
d %>% arrange(Comp.1) %>%
  left_join(iso,by=c("country"="ISO2")) %>%
  select(Comp.1,country,Country) %>%
  slice(1:8)
```

plot with country names:

```{r fig.height=15, fig.width=15}
d %>% left_join(iso,by=c("country"="ISO2")) %>%
  select(Comp.1,Comp.2,Country) %>%
  ggplot(aes(x=Comp.1,y=Comp.2,label=Country))+
  geom_point()+geom_text_repel()+
  coord_fixed()
```

with the full country names, there is more overwriting.

### principal components from correlation matrix

store in file and read in:

```{r}
my_url="http://www.utsc.utoronto.ca/~butler/d29/cov.txt"
mat=read_table(my_url,col_names=F)
mat
```

correlations between three variables X1-X3.

```{r}
mat %>% as.matrix() %>%
princomp(covmat=.) -> mat.pc
summary(mat.pc)
```

there is one component, which we can also see with a screeplot:

```{r}
ggscreeplot(mat.pc)
```

Clear elbow at 2, so one component.

Compare original correlation matrix:

```{r}
mat
```

with loadings

```{r}
mat.pc$loadings
```

component 1 is large when X1 and X2 are large, X3 small. Small for opposite.

Correlation matrix says that one of these is likely to happen, so data are basically one-dimensional.

see slide 579.